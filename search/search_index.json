{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to dsRAG","text":"<p>dsRAG is a retrieval engine for unstructured data. It is especially good at handling challenging queries over dense text, like financial reports, legal documents, and academic papers. dsRAG achieves substantially higher accuracy than vanilla RAG baselines on complex open-book question answering tasks. On one especially challenging benchmark, FinanceBench, dsRAG gets accurate answers 96.6% of the time, compared to the vanilla RAG baseline which only gets 32% of questions correct.</p>"},{"location":"#key-methods","title":"Key Methods","text":"<p>dsRAG uses three key methods to improve performance over vanilla RAG systems:</p>"},{"location":"#1-semantic-sectioning","title":"1. Semantic Sectioning","text":"<p>Semantic sectioning uses an LLM to break a document into sections. It works by annotating the document with line numbers and then prompting an LLM to identify the starting and ending lines for each \"semantically cohesive section.\" These sections should be anywhere from a few paragraphs to a few pages long. The sections then get broken into smaller chunks if needed. The LLM is also prompted to generate descriptive titles for each section. These section titles get used in the contextual chunk headers created by AutoContext, which provides additional context to the ranking models (embeddings and reranker), enabling better retrieval.</p>"},{"location":"#2-autocontext","title":"2. AutoContext","text":"<p>AutoContext creates contextual chunk headers that contain document-level and section-level context, and prepends those chunk headers to the chunks prior to embedding them. This gives the embeddings a much more accurate and complete representation of the content and meaning of the text. In our testing, this feature leads to a dramatic improvement in retrieval quality. In addition to increasing the rate at which the correct information is retrieved, AutoContext also substantially reduces the rate at which irrelevant results show up in the search results. This reduces the rate at which the LLM misinterprets a piece of text in downstream chat and generation applications.</p>"},{"location":"#3-relevant-segment-extraction-rse","title":"3. Relevant Segment Extraction (RSE)","text":"<p>Relevant Segment Extraction (RSE) is a query-time post-processing step that takes clusters of relevant chunks and intelligently combines them into longer sections of text that we call segments. These segments provide better context to the LLM than any individual chunk can. For simple factual questions, the answer is usually contained in a single chunk; but for more complex questions, the answer usually spans a longer section of text. The goal of RSE is to intelligently identify the section(s) of text that provide the most relevant information, without being constrained to fixed length chunks.</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from dsrag.create_kb import create_kb_from_file\n\n# Create a knowledge base from a file\nfile_path = \"path/to/your/document.pdf\"\nkb_id = \"my_knowledge_base\"\nkb = create_kb_from_file(kb_id, file_path)\n\n# Query the knowledge base\nsearch_queries = [\"What are the main topics covered?\"]\nresults = kb.query(search_queries)\nfor segment in results:\n    print(segment)\n</code></pre>"},{"location":"#evaluation-results","title":"Evaluation Results","text":""},{"location":"#financebench","title":"FinanceBench","text":"<p>On FinanceBench, which uses a corpus of hundreds of 10-Ks and 10-Qs with challenging queries that often require combining multiple pieces of information:</p> <ul> <li>Baseline retrieval pipeline: 32% accuracy</li> <li>dsRAG (with default parameters and Claude 3.5 Sonnet): 96.6% accuracy</li> </ul>"},{"location":"#kite-benchmark","title":"KITE Benchmark","text":"<p>On the KITE benchmark, which includes diverse datasets (AI papers, company 10-Ks, company handbooks, and Supreme Court opinions), dsRAG shows significant improvements:</p> Top-k RSE CCH+Top-k CCH+RSE AI Papers 4.5 7.9 4.7 7.9 BVP Cloud 2.6 4.4 6.3 7.8 Sourcegraph 5.7 6.6 5.8 9.4 Supreme Court Opinions 6.1 8.0 7.4 8.5 Average 4.72 6.73 6.04 8.42"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out our Quick Start Guide to begin using dsRAG in your projects.</p>"},{"location":"#community-and-support","title":"Community and Support","text":"<ul> <li>Join our Discord for community support</li> <li>Fill out our use case form if using dsRAG in production</li> <li>Need professional help? Contact our team </li> </ul>"},{"location":"api/chat/","title":"Chat","text":"<p>The Chat module provides functionality for managing chat-based interactions with knowledge bases. It handles chat thread creation, message history, and generating responses using knowledge base search.</p>"},{"location":"api/chat/#public-methods","title":"Public Methods","text":""},{"location":"api/chat/#create_new_chat_thread","title":"create_new_chat_thread","text":"<pre><code>def create_new_chat_thread(chat_thread_params: ChatThreadParams, chat_thread_db: ChatThreadDB) -&gt; str\n</code></pre> <p>Create a new chat thread in the database.</p> <p>Arguments: - <code>chat_thread_params</code>: Parameters for the chat thread. Example:   <pre><code>{\n    # Knowledge base IDs to use\n    \"kb_ids\": [\"kb1\", \"kb2\"],\n\n    # LLM model to use\n    \"model\": \"gpt-4o-mini\",\n\n    # Temperature for LLM sampling\n    \"temperature\": 0.2,\n\n    # System message for LLM\n    \"system_message\": \"You are a helpful assistant\",\n\n    # Model for auto-query generation\n    \"auto_query_model\": \"gpt-4o-mini\",\n\n    # Guidance for auto-query generation\n    \"auto_query_guidance\": \"\",\n\n    # Target response length (short/medium/long)\n    \"target_output_length\": \"medium\",\n\n    # Maximum tokens in chat history\n    \"max_chat_history_tokens\": 8000,\n\n    # Optional supplementary ID\n    \"supp_id\": \"\"\n}\n</code></pre> - <code>chat_thread_db</code>: Database instance for storing chat threads.</p> <p>Returns: Unique identifier (str) for the created chat thread.</p>"},{"location":"api/chat/#get_chat_thread_response","title":"get_chat_thread_response","text":"<pre><code>def get_chat_thread_response(thread_id: str, get_response_input: ChatResponseInput, \n                           chat_thread_db: ChatThreadDB, knowledge_bases: dict)\n</code></pre> <p>Get a response for a chat thread using knowledge base search.</p> <p>Arguments: - <code>thread_id</code>: Unique identifier for the chat thread. - <code>get_response_input</code>: Input parameters containing:   - <code>user_input</code>: User's message text   - <code>chat_thread_params</code>: Optional parameter overrides   - <code>metadata_filter</code>: Optional search filter - <code>chat_thread_db</code>: Database instance for chat threads. - <code>knowledge_bases</code>: Dictionary mapping knowledge base IDs to instances.</p> <p>Returns: Formatted interaction containing: - <code>user_input</code>: User message with content and timestamp - <code>model_response</code>: Model response with content and timestamp - <code>search_queries</code>: Generated search queries - <code>relevant_segments</code>: Retrieved relevant segments with file names and types - <code>message</code>: Error message if something went wrong (optional)</p>"},{"location":"api/chat/#chat-types","title":"Chat Types","text":""},{"location":"api/chat/#chatthreadparams","title":"ChatThreadParams","text":"<p>Type definition for chat thread parameters.</p> <pre><code>ChatThreadParams = TypedDict('ChatThreadParams', {\n    'kb_ids': List[str],              # List of knowledge base IDs to use\n    'model': str,                     # LLM model name (e.g., \"gpt-4o-mini\")\n    'temperature': float,             # Temperature for LLM sampling (0.0-1.0)\n    'system_message': str,            # Custom system message for LLM\n    'auto_query_model': str,          # Model for generating search queries\n    'auto_query_guidance': str,       # Custom guidance for query generation\n    'target_output_length': str,      # Response length (\"short\", \"medium\", \"long\")\n    'max_chat_history_tokens': int,   # Maximum tokens in chat history\n    'thread_id': str,                 # Unique thread identifier (auto-generated)\n    'supp_id': str,                   # Optional supplementary identifier\n})\n</code></pre>"},{"location":"api/chat/#chatresponseinput","title":"ChatResponseInput","text":"<p>Input parameters for getting a chat response.</p> <pre><code>ChatResponseInput = TypedDict('ChatResponseInput', {\n    'user_input': str,                          # User's message text\n    'chat_thread_params': Optional[ChatThreadParams],  # Optional parameter overrides\n    'metadata_filter': Optional[MetadataFilter], # Optional search filter\n})\n</code></pre>"},{"location":"api/chat/#metadatafilter","title":"MetadataFilter","text":"<p>Filter criteria for knowledge base searches.</p> <pre><code>MetadataFilter = TypedDict('MetadataFilter', {\n    'field_name': str,       # Name of the metadata field to filter on\n    'field_value': Any,      # Value to match against\n    'comparison_type': str,  # Type of comparison (\"equals\", \"contains\", etc.)\n}, total=False)\n</code></pre> <p>You can use these types to ensure type safety when working with the chat functions. For example:</p> <pre><code># Create chat thread parameters\nparams: ChatThreadParams = {\n    \"kb_ids\": [\"kb1\"],\n    \"model\": \"gpt-4o-mini\",\n    \"temperature\": 0.2,\n    \"system_message\": \"You are a helpful assistant\",\n    \"auto_query_model\": \"gpt-4o-mini\",\n    \"auto_query_guidance\": \"\",\n    \"target_output_length\": \"medium\",\n    \"max_chat_history_tokens\": 8000,\n    \"supp_id\": \"\"\n}\n\n# Create chat response input\nresponse_input: ChatResponseInput = {\n    \"user_input\": \"What is the capital of France?\",\n    \"chat_thread_params\": None,\n    \"metadata_filter\": None\n}\n</code></pre>"},{"location":"api/knowledge_base/","title":"KnowledgeBase","text":"<p>The KnowledgeBase class is the main interface for working with dsRAG. It handles document processing, storage, and retrieval.</p>"},{"location":"api/knowledge_base/#public-methods","title":"Public Methods","text":"<p>The following methods are part of the public API:</p> <ul> <li><code>__init__</code>: Initialize a new KnowledgeBase instance</li> <li><code>add_document</code>: Add a single document to the knowledge base</li> <li><code>add_documents</code>: Add multiple documents in parallel</li> <li><code>delete</code>: Delete the entire knowledge base and all associated data</li> <li><code>delete_document</code>: Delete a specific document from the knowledge base</li> <li><code>query</code>: Search the knowledge base with one or more queries</li> </ul> <p>Initialize a KnowledgeBase instance.</p> PARAMETER DESCRIPTION <code>kb_id</code> <p>Unique identifier for the knowledge base.</p> <p> TYPE: <code>str</code> </p> <code>title</code> <p>Title of the knowledge base. Defaults to \"\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>supp_id</code> <p>Supplementary identifier. Defaults to \"\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>description</code> <p>Description of the knowledge base. Defaults to \"\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>language</code> <p>Language code for the knowledge base. Defaults to \"en\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>storage_directory</code> <p>Base directory for storing files. Defaults to \"~/dsRAG\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'~/dsRAG'</code> </p> <code>embedding_model</code> <p>Model for generating embeddings.  Defaults to OpenAIEmbedding.</p> <p> TYPE: <code>Optional[Embedding]</code> DEFAULT: <code>None</code> </p> <code>reranker</code> <p>Model for reranking results.  Defaults to CohereReranker.</p> <p> TYPE: <code>Optional[Reranker]</code> DEFAULT: <code>None</code> </p> <code>auto_context_model</code> <p>LLM for generating context.  Defaults to OpenAIChatAPI.</p> <p> TYPE: <code>Optional[LLM]</code> DEFAULT: <code>None</code> </p> <code>vector_db</code> <p>Vector database for storing embeddings.  Defaults to BasicVectorDB.</p> <p> TYPE: <code>Optional[VectorDB]</code> DEFAULT: <code>None</code> </p> <code>chunk_db</code> <p>Database for storing text chunks.  Defaults to BasicChunkDB.</p> <p> TYPE: <code>Optional[ChunkDB]</code> DEFAULT: <code>None</code> </p> <code>file_system</code> <p>File system for storing images.  Defaults to LocalFileSystem.</p> <p> TYPE: <code>Optional[FileSystem]</code> DEFAULT: <code>None</code> </p> <code>exists_ok</code> <p>Whether to load existing KB if it exists. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>save_metadata_to_disk</code> <p>Whether to persist metadata. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>metadata_storage</code> <p>Storage for KB metadata.  Defaults to LocalMetadataStorage.</p> <p> TYPE: <code>Optional[MetadataStorage]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If KB exists and exists_ok is False.</p> Source code in <code>dsrag/knowledge_base.py</code> <pre><code>def __init__(\n    self,\n    kb_id: str,\n    title: str = \"\",\n    supp_id: str = \"\",\n    description: str = \"\",\n    language: str = \"en\",\n    storage_directory: str = \"~/dsRAG\",\n    embedding_model: Optional[Embedding] = None,\n    reranker: Optional[Reranker] = None,\n    auto_context_model: Optional[LLM] = None,\n    vector_db: Optional[VectorDB] = None,\n    chunk_db: Optional[ChunkDB] = None,\n    file_system: Optional[FileSystem] = None,\n    exists_ok: bool = True,\n    save_metadata_to_disk: bool = True,\n    metadata_storage: Optional[MetadataStorage] = None\n):\n    \"\"\"Initialize a KnowledgeBase instance.\n\n    Args:\n        kb_id (str): Unique identifier for the knowledge base.\n        title (str, optional): Title of the knowledge base. Defaults to \"\".\n        supp_id (str, optional): Supplementary identifier. Defaults to \"\".\n        description (str, optional): Description of the knowledge base. Defaults to \"\".\n        language (str, optional): Language code for the knowledge base. Defaults to \"en\".\n        storage_directory (str, optional): Base directory for storing files. Defaults to \"~/dsRAG\".\n        embedding_model (Optional[Embedding], optional): Model for generating embeddings. \n            Defaults to OpenAIEmbedding.\n        reranker (Optional[Reranker], optional): Model for reranking results. \n            Defaults to CohereReranker.\n        auto_context_model (Optional[LLM], optional): LLM for generating context. \n            Defaults to OpenAIChatAPI.\n        vector_db (Optional[VectorDB], optional): Vector database for storing embeddings. \n            Defaults to BasicVectorDB.\n        chunk_db (Optional[ChunkDB], optional): Database for storing text chunks. \n            Defaults to BasicChunkDB.\n        file_system (Optional[FileSystem], optional): File system for storing images. \n            Defaults to LocalFileSystem.\n        exists_ok (bool, optional): Whether to load existing KB if it exists. Defaults to True.\n        save_metadata_to_disk (bool, optional): Whether to persist metadata. Defaults to True.\n        metadata_storage (Optional[MetadataStorage], optional): Storage for KB metadata. \n            Defaults to LocalMetadataStorage.\n\n    Raises:\n        ValueError: If KB exists and exists_ok is False.\n    \"\"\"\n    self.kb_id = kb_id\n    self.storage_directory = os.path.expanduser(storage_directory)\n    self.metadata_storage = metadata_storage if metadata_storage else LocalMetadataStorage(self.storage_directory)\n\n    if save_metadata_to_disk:\n        # load the KB if it exists; otherwise, initialize it and save it to disk\n        if self.metadata_storage.kb_exists(self.kb_id) and exists_ok:\n            self._load(\n                auto_context_model, reranker, file_system, chunk_db\n            )\n            self._save()\n        elif self.metadata_storage.kb_exists(self.kb_id) and not exists_ok:\n            raise ValueError(\n                f\"Knowledge Base with ID {kb_id} already exists. Use exists_ok=True to load it.\"\n            )\n        else:\n            created_time = int(time.time())\n            self.kb_metadata = {\n                \"title\": title,\n                \"description\": description,\n                \"language\": language,\n                \"supp_id\": supp_id,\n                \"created_on\": created_time,\n            }\n            self._initialize_components(\n                embedding_model, reranker, auto_context_model, vector_db, chunk_db, file_system\n            )\n            self._save()  # save the config for the KB to disk\n    else:\n        self.kb_metadata = {\n            \"title\": title,\n            \"description\": description,\n            \"language\": language,\n            \"supp_id\": supp_id,\n        }\n        self._initialize_components(\n            embedding_model, reranker, auto_context_model, vector_db, chunk_db, file_system\n        )\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.knowledge_base.KnowledgeBase-functions","title":"Functions","text":""},{"location":"api/knowledge_base/#dsrag.knowledge_base.KnowledgeBase.add_document","title":"add_document","text":"<pre><code>add_document(doc_id: str, text: str = '', file_path: str = '', document_title: str = '', auto_context_config: dict = {}, file_parsing_config: dict = {}, semantic_sectioning_config: dict = {}, chunking_config: dict = {}, chunk_size: int = None, min_length_for_chunking: int = None, supp_id: str = '', metadata: dict = {})\n</code></pre> <p>Add a document to the knowledge base.</p> <p>This method processes and adds a document to the knowledge base. The document can be provided either as text or as a file path. The document will be processed according to the provided configuration parameters.</p> PARAMETER DESCRIPTION <code>doc_id</code> <p>Unique identifier for the document. A file name or path is a good choice.</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>The full text of the document. Defaults to \"\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>file_path</code> <p>Path to the file to be uploaded. Supported file types are .txt, .md, .pdf, and .docx. Defaults to \"\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>document_title</code> <p>The title of the document. If not provided, either the doc_id or an LLM-generated title will be used, depending on auto_context_config. Defaults to \"\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>auto_context_config</code> <p>Configuration parameters for AutoContext. Example: <pre><code>{\n    # Whether to use an LLM-generated title if no title is provided\n    \"use_generated_title\": True,\n\n    # Guidance for generating the document title\n    \"document_title_guidance\": \"Generate a concise title\",\n\n    # Whether to get a document summary\n    \"get_document_summary\": True,\n\n    # Guidance for document summarization\n    \"document_summarization_guidance\": \"Summarize key points\",\n\n    # Whether to get section summaries\n    \"get_section_summaries\": False,\n\n    # Guidance for section summarization\n    \"section_summarization_guidance\": \"Summarize each section\",\n\n    # Custom term mappings (key: term to map to, value: list of terms to map from)\n    \"custom_term_mapping\": {\n        \"AI\": [\"artificial intelligence\", \"machine learning\"],\n        \"ML\": [\"machine learning\", \"deep learning\"]\n    }\n}\n</code></pre></p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>file_parsing_config</code> <p>Configuration parameters for file parsing. Example: <pre><code>{\n    # Whether to use VLM for parsing\n    \"use_vlm\": False,\n\n    # VLM configuration (ignored if use_vlm is False)\n    \"vlm_config\": {\n        # VLM provider (currently only \"gemini\" and \"vertex_ai\" supported)\n        \"provider\": \"vertex_ai\",\n\n        # The VLM model to use\n        \"model\": \"model_name\",\n\n        # GCP project ID (required for \"vertex_ai\")\n        \"project_id\": \"your-project-id\",\n\n        # GCP location (required for \"vertex_ai\")\n        \"location\": \"us-central1\",\n\n        # Path to save intermediate files\n        \"save_path\": \"/path/to/save\",\n\n        # Element types to exclude\n        \"exclude_elements\": [\"Header\", \"Footer\"],\n\n        # Whether images are pre-extracted\n        \"images_already_exist\": False\n    },\n\n    # Save images even if VLM unused\n    \"always_save_page_images\": False\n}\n</code></pre></p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>semantic_sectioning_config</code> <p>Configuration for semantic sectioning. Example: <pre><code>{\n    # LLM provider for semantic sectioning\n    \"llm_provider\": \"openai\",  # or \"anthropic\" or \"gemini\"\n\n    # LLM model to use\n    \"model\": \"gpt-4o-mini\",\n\n    # Whether to use semantic sectioning\n    \"use_semantic_sectioning\": True\n}\n</code></pre></p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>chunking_config</code> <p>Configuration for document/section chunking. Example: <pre><code>{\n    # Maximum characters per chunk\n    \"chunk_size\": 800,\n\n    # Minimum text length to allow chunking\n    \"min_length_for_chunking\": 2000\n}\n</code></pre></p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>supp_id</code> <p>Supplementary identifier. Defaults to \"\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>metadata</code> <p>Additional metadata for the document. Defaults to {}.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> Note <p>Either text or file_path must be provided. If both are provided, text takes precedence. The document processing flow is: 1. File parsing (if file_path provided) 2. Semantic sectioning (if enabled) 3. Chunking 4. AutoContext 5. Embedding 6. Storage in vector and chunk databases</p> Source code in <code>dsrag/knowledge_base.py</code> <pre><code>def add_document(\n    self,\n    doc_id: str,\n    text: str = \"\",\n    file_path: str = \"\",\n    document_title: str = \"\",\n    auto_context_config: dict = {},\n    file_parsing_config: dict = {},\n    semantic_sectioning_config: dict = {},\n    chunking_config: dict = {},\n    chunk_size: int = None,\n    min_length_for_chunking: int = None,\n    supp_id: str = \"\",\n    metadata: dict = {},\n):\n    \"\"\"Add a document to the knowledge base.\n\n    This method processes and adds a document to the knowledge base. The document can be provided\n    either as text or as a file path. The document will be processed according to the provided\n    configuration parameters.\n\n    Args:\n        doc_id (str): Unique identifier for the document. A file name or path is a good choice.\n        text (str, optional): The full text of the document. Defaults to \"\".\n        file_path (str, optional): Path to the file to be uploaded. Supported file types are\n            .txt, .md, .pdf, and .docx. Defaults to \"\".\n        document_title (str, optional): The title of the document. If not provided, either the\n            doc_id or an LLM-generated title will be used, depending on auto_context_config.\n            Defaults to \"\".\n        auto_context_config (dict, optional): Configuration parameters for AutoContext. Example:\n            ```python\n            {\n                # Whether to use an LLM-generated title if no title is provided\n                \"use_generated_title\": True,\n\n                # Guidance for generating the document title\n                \"document_title_guidance\": \"Generate a concise title\",\n\n                # Whether to get a document summary\n                \"get_document_summary\": True,\n\n                # Guidance for document summarization\n                \"document_summarization_guidance\": \"Summarize key points\",\n\n                # Whether to get section summaries\n                \"get_section_summaries\": False,\n\n                # Guidance for section summarization\n                \"section_summarization_guidance\": \"Summarize each section\",\n\n                # Custom term mappings (key: term to map to, value: list of terms to map from)\n                \"custom_term_mapping\": {\n                    \"AI\": [\"artificial intelligence\", \"machine learning\"],\n                    \"ML\": [\"machine learning\", \"deep learning\"]\n                }\n            }\n            ```\n        file_parsing_config (dict, optional): Configuration parameters for file parsing. Example:\n            ```python\n            {\n                # Whether to use VLM for parsing\n                \"use_vlm\": False,\n\n                # VLM configuration (ignored if use_vlm is False)\n                \"vlm_config\": {\n                    # VLM provider (currently only \"gemini\" and \"vertex_ai\" supported)\n                    \"provider\": \"vertex_ai\",\n\n                    # The VLM model to use\n                    \"model\": \"model_name\",\n\n                    # GCP project ID (required for \"vertex_ai\")\n                    \"project_id\": \"your-project-id\",\n\n                    # GCP location (required for \"vertex_ai\")\n                    \"location\": \"us-central1\",\n\n                    # Path to save intermediate files\n                    \"save_path\": \"/path/to/save\",\n\n                    # Element types to exclude\n                    \"exclude_elements\": [\"Header\", \"Footer\"],\n\n                    # Whether images are pre-extracted\n                    \"images_already_exist\": False\n                },\n\n                # Save images even if VLM unused\n                \"always_save_page_images\": False\n            }\n            ```\n        semantic_sectioning_config (dict, optional): Configuration for semantic sectioning. Example:\n            ```python\n            {\n                # LLM provider for semantic sectioning\n                \"llm_provider\": \"openai\",  # or \"anthropic\" or \"gemini\"\n\n                # LLM model to use\n                \"model\": \"gpt-4o-mini\",\n\n                # Whether to use semantic sectioning\n                \"use_semantic_sectioning\": True\n            }\n            ```\n        chunking_config (dict, optional): Configuration for document/section chunking. Example:\n            ```python\n            {\n                # Maximum characters per chunk\n                \"chunk_size\": 800,\n\n                # Minimum text length to allow chunking\n                \"min_length_for_chunking\": 2000\n            }\n            ```\n        supp_id (str, optional): Supplementary identifier. Defaults to \"\".\n        metadata (dict, optional): Additional metadata for the document. Defaults to {}.\n\n    Note:\n        Either text or file_path must be provided. If both are provided, text takes precedence.\n        The document processing flow is:\n        1. File parsing (if file_path provided)\n        2. Semantic sectioning (if enabled)\n        3. Chunking\n        4. AutoContext\n        5. Embedding\n        6. Storage in vector and chunk databases\n    \"\"\"\n\n    # Handle the backwards compatibility for chunk_size and min_length_for_chunking\n    if chunk_size is not None:\n        chunking_config[\"chunk_size\"] = chunk_size\n    if min_length_for_chunking is not None:\n        chunking_config[\"min_length_for_chunking\"] = min_length_for_chunking\n\n    if text == \"\" and file_path == \"\":\n        raise ValueError(\"Either text or file_path must be provided\")\n\n    # verify that the document does not already exist in the KB - the doc_id should be unique\n    if doc_id in self.chunk_db.get_all_doc_ids():\n        print(f\"Document with ID {doc_id} already exists in the KB. Skipping...\")\n        return\n\n    # verify the doc_id is valid\n    if \"/\" in doc_id:\n        raise ValueError(\"doc_id cannot contain '/' characters\")\n\n    sections, chunks = parse_and_chunk(\n        kb_id=self.kb_id,\n        doc_id=doc_id,\n        file_path=file_path, \n        text=text, \n        file_parsing_config=file_parsing_config, \n        semantic_sectioning_config=semantic_sectioning_config, \n        chunking_config=chunking_config,\n        file_system=self.file_system,\n    )\n    chunks, chunks_to_embed = auto_context(\n        auto_context_model=self.auto_context_model, \n        sections=sections, \n        chunks=chunks, \n        text=text, \n        doc_id=doc_id, \n        document_title=document_title, \n        auto_context_config=auto_context_config, \n        language=self.kb_metadata[\"language\"],\n    )\n    chunk_embeddings = get_embeddings(\n        embedding_model=self.embedding_model,\n        chunks_to_embed=chunks_to_embed,\n    )\n    add_chunks_to_db(\n        chunk_db=self.chunk_db,\n        chunks=chunks,\n        chunks_to_embed=chunks_to_embed,\n        chunk_embeddings=chunk_embeddings,\n        metadata=metadata,\n        doc_id=doc_id,\n        supp_id=supp_id\n    )\n    add_vectors_to_db(\n        vector_db=self.vector_db,\n        chunks=chunks,\n        chunk_embeddings=chunk_embeddings,\n        metadata=metadata,\n        doc_id=doc_id,\n    )\n\n    # Convert elements to page content if the document was processed with page numbers\n    if file_path and file_parsing_config.get('use_vlm', False):\n        # NOTE: does this really need to be in a try/except block?\n        try:\n            elements = self.file_system.load_data(kb_id=self.kb_id, doc_id=doc_id, data_name=\"elements\")\n            if elements:\n                convert_elements_to_page_content(\n                    elements=elements,\n                    kb_id=self.kb_id,\n                    doc_id=doc_id,\n                    file_system=self.file_system\n                )\n        except Exception as e:\n            print(f\"Warning: Failed to load or process elements for page content: {str(e)}\")\n\n    self._save()  # save to disk after adding a document\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.knowledge_base.KnowledgeBase.add_documents","title":"add_documents","text":"<pre><code>add_documents(documents: List[Dict[str, Union[str, dict]]], max_workers: int = 1, show_progress: bool = True, rate_limit_pause: float = 1.0) -&gt; List[str]\n</code></pre> <p>Add multiple documents to the knowledge base in parallel.</p> PARAMETER DESCRIPTION <code>documents</code> <p>List of document dictionaries. Each must contain: - 'doc_id' (str): Unique identifier for the document And either: - 'text' (str): The document content, or - 'file_path' (str): Path to the document file Optional keys: - 'document_title' (str): Document title - 'auto_context_config' (dict): AutoContext configuration - 'file_parsing_config' (dict): File parsing configuration - 'semantic_sectioning_config' (dict): Semantic sectioning configuration - 'chunking_config' (dict): Chunking configuration - 'supp_id' (str): Supplementary identifier - 'metadata' (dict): Additional metadata</p> <p> TYPE: <code>List[Dict[str, Union[str, dict]]]</code> </p> <code>max_workers</code> <p>Maximum number of worker threads. Defaults to 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>show_progress</code> <p>Whether to show a progress bar. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>rate_limit_pause</code> <p>Pause between uploads in seconds. Defaults to 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>List[str]: List of successfully uploaded document IDs.</p> Note <p>Be sure to use thread-safe VectorDB and ChunkDB implementations when max_workers &gt; 1. The default implementations (BasicVectorDB and BasicChunkDB) are not thread-safe.</p> Source code in <code>dsrag/knowledge_base.py</code> <pre><code>def add_documents(\n    self,\n    documents: List[Dict[str, Union[str, dict]]],\n    max_workers: int = 1,\n    show_progress: bool = True,\n    rate_limit_pause: float = 1.0,\n) -&gt; List[str]:\n    \"\"\"Add multiple documents to the knowledge base in parallel.\n\n    Args:\n        documents (List[Dict[str, Union[str, dict]]]): List of document dictionaries. Each must contain:\n            - 'doc_id' (str): Unique identifier for the document\n            And either:\n            - 'text' (str): The document content, or\n            - 'file_path' (str): Path to the document file\n            Optional keys:\n            - 'document_title' (str): Document title\n            - 'auto_context_config' (dict): AutoContext configuration\n            - 'file_parsing_config' (dict): File parsing configuration\n            - 'semantic_sectioning_config' (dict): Semantic sectioning configuration\n            - 'chunking_config' (dict): Chunking configuration\n            - 'supp_id' (str): Supplementary identifier\n            - 'metadata' (dict): Additional metadata\n        max_workers (int, optional): Maximum number of worker threads. Defaults to 1.\n        show_progress (bool, optional): Whether to show a progress bar. Defaults to True.\n        rate_limit_pause (float, optional): Pause between uploads in seconds. Defaults to 1.0.\n\n    Returns:\n        List[str]: List of successfully uploaded document IDs.\n\n    Note:\n        Be sure to use thread-safe VectorDB and ChunkDB implementations when max_workers &gt; 1.\n        The default implementations (BasicVectorDB and BasicChunkDB) are not thread-safe.\n    \"\"\"\n    successful_uploads = []\n\n    def process_document(doc: Dict) -&gt; Optional[str]:\n        try:\n            # Extract required parameters\n            doc_id = doc['doc_id']\n            print(f\"Starting to process document: {doc_id}\")  # Debug log\n\n            # Create a copy of the document dict to avoid modification during iteration\n            doc_params = doc.copy()\n\n            # Extract required parameters from the copy\n            text = doc_params.get('text', '')\n            file_path = doc_params.get('file_path', '')\n\n            # Extract optional parameters with defaults\n            document_title = doc_params.get('document_title', '')\n            auto_context_config = doc_params.get('auto_context_config', {}).copy()\n            file_parsing_config = doc_params.get('file_parsing_config', {}).copy()\n            semantic_sectioning_config = doc_params.get('semantic_sectioning_config', {}).copy()\n            chunking_config = doc_params.get('chunking_config', {}).copy()\n            supp_id = doc_params.get('supp_id', '')\n            metadata = doc_params.get('metadata', {}).copy()\n\n            print(f\"Extracted parameters for {doc_id}\")  # Debug log\n\n            # Call add_document with extracted parameters\n            self.add_document(\n                doc_id=doc_id,\n                text=text,\n                file_path=file_path,\n                document_title=document_title,\n                auto_context_config=auto_context_config,\n                file_parsing_config=file_parsing_config,\n                semantic_sectioning_config=semantic_sectioning_config,\n                chunking_config=chunking_config,\n                supp_id=supp_id,\n                metadata=metadata\n            )\n\n            print(f\"Successfully processed document: {doc_id}\")  # Debug log\n\n            # Pause to avoid rate limits\n            time.sleep(rate_limit_pause)\n            return doc_id\n\n        except Exception as e:\n            import traceback\n            error_msg = f\"Error processing document {doc.get('doc_id', 'unknown')}:\\n\"\n            error_msg += f\"Error type: {type(e).__name__}\\n\"\n            error_msg += f\"Error message: {str(e)}\\n\"\n            error_msg += \"Traceback:\\n\"\n            error_msg += traceback.format_exc()\n            print(error_msg)\n            return None\n\n    # Process documents in parallel\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        # Create futures\n        future_to_doc = {\n            executor.submit(process_document, doc): doc \n            for doc in documents\n        }\n\n        # Process results with optional progress bar\n        if show_progress:\n            futures = tqdm(\n                concurrent.futures.as_completed(future_to_doc),\n                total=len(documents),\n                desc=\"Processing documents\"\n            )\n        else:\n            futures = concurrent.futures.as_completed(future_to_doc)\n\n        for future in futures:\n            doc_id = future.result()\n            if doc_id:\n                successful_uploads.append(doc_id)\n\n    return successful_uploads\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.knowledge_base.KnowledgeBase.delete","title":"delete","text":"<pre><code>delete()\n</code></pre> <p>Delete the knowledge base and all associated data.</p> <p>Removes all documents, vectors, chunks, and metadata associated with this KB.</p> Source code in <code>dsrag/knowledge_base.py</code> <pre><code>def delete(self):\n    \"\"\"Delete the knowledge base and all associated data.\n\n    Removes all documents, vectors, chunks, and metadata associated with this KB.\n    \"\"\"\n    # delete all documents in the KB\n    doc_ids_to_delete = self.chunk_db.get_all_doc_ids()\n    for doc_id in doc_ids_to_delete:\n        self.delete_document(doc_id)\n\n    self.chunk_db.delete()\n    self.vector_db.delete()\n    self.file_system.delete_kb(self.kb_id)\n\n    # delete the metadata file\n    self.metadata_storage.delete(self.kb_id)\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.knowledge_base.KnowledgeBase.delete_document","title":"delete_document","text":"<pre><code>delete_document(doc_id: str)\n</code></pre> <p>Delete a document from the knowledge base.</p> PARAMETER DESCRIPTION <code>doc_id</code> <p>ID of the document to delete.</p> <p> TYPE: <code>str</code> </p> Source code in <code>dsrag/knowledge_base.py</code> <pre><code>def delete_document(self, doc_id: str):\n    \"\"\"Delete a document from the knowledge base.\n\n    Args:\n        doc_id (str): ID of the document to delete.\n    \"\"\"\n    self.chunk_db.remove_document(doc_id)\n    self.vector_db.remove_document(doc_id)\n    self.file_system.delete_directory(self.kb_id, doc_id)\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.knowledge_base.KnowledgeBase.query","title":"query","text":"<pre><code>query(search_queries: list[str], rse_params: Union[Dict, str] = 'balanced', latency_profiling: bool = False, metadata_filter: Optional[MetadataFilter] = None, return_mode: str = 'text') -&gt; list[dict]\n</code></pre> <p>Query the knowledge base to retrieve relevant segments.</p> PARAMETER DESCRIPTION <code>search_queries</code> <p>List of search queries to execute.</p> <p> TYPE: <code>list[str]</code> </p> <code>rse_params</code> <p>RSE parameters or preset name. Example: <pre><code>{\n    # Maximum segment length in chunks\n    \"max_length\": 5,\n\n    # Maximum total length of all segments\n    \"overall_max_length\": 20,\n\n    # Minimum relevance value for segments\n    \"minimum_value\": 0.5,\n\n    # Penalty for irrelevant chunks (0-1)\n    \"irrelevant_chunk_penalty\": 0.8,\n\n    # Length increase per additional query\n    \"overall_max_length_extension\": 5,\n\n    # Rate at which relevance decays\n    \"decay_rate\": 0.1,\n\n    # Number of documents to consider\n    \"top_k_for_document_selection\": 10,\n\n    # Whether to scale by chunk length\n    \"chunk_length_adjustment\": True\n}\n</code></pre> Alternatively, use preset names: \"balanced\" (default), \"precise\", or \"comprehensive\"</p> <p> TYPE: <code>Union[Dict, str]</code> DEFAULT: <code>'balanced'</code> </p> <code>latency_profiling</code> <p>Whether to print timing info. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>metadata_filter</code> <p>Filter for document selection.  Defaults to None.</p> <p> TYPE: <code>Optional[MetadataFilter]</code> DEFAULT: <code>None</code> </p> <code>return_mode</code> <p>Content return format. One of: - \"text\": Return segments as text - \"page_images\": Return list of page image paths - \"dynamic\": Choose format based on content type Defaults to \"text\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'text'</code> </p> RETURNS DESCRIPTION <code>list[dict]</code> <p>list[dict]: List of segment information dictionaries, ordered by relevance. Each dictionary contains: <pre><code>{\n    # Document identifier\n    \"doc_id\": \"example_doc\",\n\n    # Starting chunk index\n    \"chunk_start\": 0,\n\n    # Ending chunk index (exclusive)\n    \"chunk_end\": 5,\n\n    # Segment content (text or image paths)\n    \"content\": \"Example text content...\",\n\n    # Starting page number\n    \"segment_page_start\": 1,\n\n    # Ending page number\n    \"segment_page_end\": 3,\n\n    # Relevance score\n    \"score\": 0.95\n}\n</code></pre></p> Source code in <code>dsrag/knowledge_base.py</code> <pre><code>def query(\n    self,\n    search_queries: list[str],\n    rse_params: Union[Dict, str] = \"balanced\",\n    latency_profiling: bool = False,\n    metadata_filter: Optional[MetadataFilter] = None,\n    return_mode: str = \"text\",\n) -&gt; list[dict]:\n    \"\"\"Query the knowledge base to retrieve relevant segments.\n\n    Args:\n        search_queries (list[str]): List of search queries to execute.\n        rse_params (Union[Dict, str], optional): RSE parameters or preset name. Example:\n            ```python\n            {\n                # Maximum segment length in chunks\n                \"max_length\": 5,\n\n                # Maximum total length of all segments\n                \"overall_max_length\": 20,\n\n                # Minimum relevance value for segments\n                \"minimum_value\": 0.5,\n\n                # Penalty for irrelevant chunks (0-1)\n                \"irrelevant_chunk_penalty\": 0.8,\n\n                # Length increase per additional query\n                \"overall_max_length_extension\": 5,\n\n                # Rate at which relevance decays\n                \"decay_rate\": 0.1,\n\n                # Number of documents to consider\n                \"top_k_for_document_selection\": 10,\n\n                # Whether to scale by chunk length\n                \"chunk_length_adjustment\": True\n            }\n            ```\n            Alternatively, use preset names: \"balanced\" (default), \"precise\", or \"comprehensive\"\n        latency_profiling (bool, optional): Whether to print timing info. Defaults to False.\n        metadata_filter (Optional[MetadataFilter], optional): Filter for document selection. \n            Defaults to None.\n        return_mode (str, optional): Content return format. One of:\n            - \"text\": Return segments as text\n            - \"page_images\": Return list of page image paths\n            - \"dynamic\": Choose format based on content type\n            Defaults to \"text\".\n\n    Returns:\n        list[dict]: List of segment information dictionaries, ordered by relevance.\n            Each dictionary contains:\n            ```python\n            {\n                # Document identifier\n                \"doc_id\": \"example_doc\",\n\n                # Starting chunk index\n                \"chunk_start\": 0,\n\n                # Ending chunk index (exclusive)\n                \"chunk_end\": 5,\n\n                # Segment content (text or image paths)\n                \"content\": \"Example text content...\",\n\n                # Starting page number\n                \"segment_page_start\": 1,\n\n                # Ending page number\n                \"segment_page_end\": 3,\n\n                # Relevance score\n                \"score\": 0.95\n            }\n            ```\n    \"\"\"\n    # check if the rse_params is a preset name and convert it to a dictionary if it is\n    if isinstance(rse_params, str) and rse_params in RSE_PARAMS_PRESETS:\n        rse_params = RSE_PARAMS_PRESETS[rse_params]\n    elif isinstance(rse_params, str):\n        raise ValueError(f\"Invalid rse_params preset name: {rse_params}\")\n\n    # set the RSE parameters - use the 'balanced' preset as the default for any missing parameters\n    default_rse_params = RSE_PARAMS_PRESETS[\"balanced\"]\n    max_length = rse_params.get(\"max_length\", default_rse_params[\"max_length\"])\n    overall_max_length = rse_params.get(\n        \"overall_max_length\", default_rse_params[\"overall_max_length\"]\n    )\n    minimum_value = rse_params.get(\n        \"minimum_value\", default_rse_params[\"minimum_value\"]\n    )\n    irrelevant_chunk_penalty = rse_params.get(\n        \"irrelevant_chunk_penalty\", default_rse_params[\"irrelevant_chunk_penalty\"]\n    )\n    overall_max_length_extension = rse_params.get(\n        \"overall_max_length_extension\",\n        default_rse_params[\"overall_max_length_extension\"],\n    )\n    decay_rate = rse_params.get(\"decay_rate\", default_rse_params[\"decay_rate\"])\n    top_k_for_document_selection = rse_params.get(\n        \"top_k_for_document_selection\",\n        default_rse_params[\"top_k_for_document_selection\"],\n    )\n    chunk_length_adjustment = rse_params.get(\n        \"chunk_length_adjustment\", default_rse_params[\"chunk_length_adjustment\"]\n    )\n\n    overall_max_length += (\n        len(search_queries) - 1\n    ) * overall_max_length_extension  # increase the overall max length for each additional query\n\n    start_time = time.time()\n    all_ranked_results = self._get_all_ranked_results(search_queries=search_queries, metadata_filter=metadata_filter)\n    if latency_profiling:\n        print(\n            f\"get_all_ranked_results took {time.time() - start_time} seconds to run for {len(search_queries)} queries\"\n        )\n\n    document_splits, document_start_points, unique_document_ids = get_meta_document(\n        all_ranked_results=all_ranked_results,\n        top_k_for_document_selection=top_k_for_document_selection,\n    )\n\n    # verify that we have a valid meta-document - otherwise return an empty list of segments\n    if len(document_splits) == 0:\n        return []\n\n    # get the length of the meta-document so we don't have to pass in the whole list of splits\n    meta_document_length = document_splits[-1]\n\n    # get the relevance values for each chunk in the meta-document and use those to find the best segments\n    all_relevance_values = get_relevance_values(\n        all_ranked_results=all_ranked_results,\n        meta_document_length=meta_document_length,\n        document_start_points=document_start_points,\n        unique_document_ids=unique_document_ids,\n        irrelevant_chunk_penalty=irrelevant_chunk_penalty,\n        decay_rate=decay_rate,\n        chunk_length_adjustment=chunk_length_adjustment,\n    )\n    best_segments, scores = get_best_segments(\n        all_relevance_values=all_relevance_values,\n        document_splits=document_splits,\n        max_length=max_length,\n        overall_max_length=overall_max_length,\n        minimum_value=minimum_value,\n    )\n\n    # convert the best segments into a list of dictionaries that contain the document id and the start and end of the chunk\n    relevant_segment_info = []\n    for segment_index, (start, end) in enumerate(best_segments):\n        # find the document that this segment starts in\n        for i, split in enumerate(document_splits):\n            if start &lt; split:  # splits represent the end of each document\n                doc_start = document_splits[i - 1] if i &gt; 0 else 0\n                relevant_segment_info.append(\n                    {\n                        \"doc_id\": unique_document_ids[i],\n                        \"chunk_start\": start - doc_start,\n                        \"chunk_end\": end - doc_start,\n                    }\n                )  # NOTE: end index is non-inclusive\n                break\n\n        score = scores[segment_index]\n        relevant_segment_info[-1][\"score\"] = score\n\n    # retrieve the content for each of the segments\n    for segment_info in relevant_segment_info:\n        segment_info[\"content\"] = self._get_segment_content_from_database(\n            segment_info[\"doc_id\"],\n            segment_info[\"chunk_start\"],\n            segment_info[\"chunk_end\"],\n            return_mode=return_mode,\n        )\n        start_page_number, end_page_number = self._get_segment_page_numbers(\n            segment_info[\"doc_id\"],\n            segment_info[\"chunk_start\"],\n            segment_info[\"chunk_end\"]\n        )\n        segment_info[\"segment_page_start\"] = start_page_number\n        segment_info[\"segment_page_end\"] = end_page_number\n\n        # Deprecated keys, but needed for backwards compatibility\n        segment_info[\"chunk_page_start\"] = start_page_number\n        segment_info[\"chunk_page_end\"] = end_page_number\n\n        # Backwards compatibility, where previously the content was stored in the \"text\" key\n        if type(segment_info[\"content\"]) == str:\n            segment_info[\"text\"] = segment_info[\"content\"]\n        else:\n            segment_info[\"text\"] = \"\"\n\n    return relevant_segment_info\n</code></pre>"},{"location":"api/knowledge_base/#kb-components","title":"KB Components","text":""},{"location":"api/knowledge_base/#vector-databases","title":"Vector Databases","text":""},{"location":"api/knowledge_base/#dsrag.database.vector.VectorDB","title":"VectorDB","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api/knowledge_base/#dsrag.database.vector.VectorDB-functions","title":"Functions","text":""},{"location":"api/knowledge_base/#dsrag.database.vector.VectorDB.add_vectors","title":"add_vectors  <code>abstractmethod</code>","text":"<pre><code>add_vectors(vectors: Sequence[Vector], metadata: Sequence[ChunkMetadata]) -&gt; None\n</code></pre> <p>Store a list of vectors with associated metadata.</p> Source code in <code>dsrag/database/vector/db.py</code> <pre><code>@abstractmethod\ndef add_vectors(\n    self, vectors: Sequence[Vector], metadata: Sequence[ChunkMetadata]\n) -&gt; None:\n    \"\"\"\n    Store a list of vectors with associated metadata.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.vector.VectorDB.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Delete the vector database.</p> Source code in <code>dsrag/database/vector/db.py</code> <pre><code>@abstractmethod\ndef delete(self) -&gt; None:\n    \"\"\"\n    Delete the vector database.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.vector.VectorDB.remove_document","title":"remove_document  <code>abstractmethod</code>","text":"<pre><code>remove_document(doc_id) -&gt; None\n</code></pre> <p>Remove all vectors and metadata associated with a given document ID.</p> Source code in <code>dsrag/database/vector/db.py</code> <pre><code>@abstractmethod\ndef remove_document(self, doc_id) -&gt; None:\n    \"\"\"\n    Remove all vectors and metadata associated with a given document ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.vector.VectorDB.search","title":"search  <code>abstractmethod</code>","text":"<pre><code>search(query_vector, top_k: int = 10, metadata_filter: Optional[dict] = None) -&gt; list[VectorSearchResult]\n</code></pre> <p>Retrieve the top-k closest vectors to a given query vector. - needs to return results as list of dictionaries in this format: {     'metadata':         {             'doc_id': doc_id,             'chunk_index': chunk_index,             'chunk_header': chunk_header,             'chunk_text': chunk_text         },     'similarity': similarity, }</p> Source code in <code>dsrag/database/vector/db.py</code> <pre><code>@abstractmethod\ndef search(self, query_vector, top_k: int=10, metadata_filter: Optional[dict] = None) -&gt; list[VectorSearchResult]:\n    \"\"\"\n    Retrieve the top-k closest vectors to a given query vector.\n    - needs to return results as list of dictionaries in this format:\n    {\n        'metadata':\n            {\n                'doc_id': doc_id,\n                'chunk_index': chunk_index,\n                'chunk_header': chunk_header,\n                'chunk_text': chunk_text\n            },\n        'similarity': similarity,\n    }\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#chunk-databases","title":"Chunk Databases","text":""},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB","title":"ChunkDB","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB-functions","title":"Functions","text":""},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.add_document","title":"add_document  <code>abstractmethod</code>","text":"<pre><code>add_document(doc_id: str, chunks: dict[int, dict[str, Any]], supp_id: str = '', metadata: dict = {}) -&gt; None\n</code></pre> <p>Store all chunks for a given document.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef add_document(self, doc_id: str, chunks: dict[int, dict[str, Any]], supp_id: str = \"\", metadata: dict = {}) -&gt; None:\n    \"\"\"\n    Store all chunks for a given document.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Delete the chunk database.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef delete(self) -&gt; None:\n    \"\"\"\n    Delete the chunk database.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.get_all_doc_ids","title":"get_all_doc_ids  <code>abstractmethod</code>","text":"<pre><code>get_all_doc_ids(supp_id: Optional[str] = None) -&gt; list[str]\n</code></pre> <p>Retrieve all document IDs.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef get_all_doc_ids(self, supp_id: Optional[str] = None) -&gt; list[str]:\n    \"\"\"\n    Retrieve all document IDs.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.get_chunk_page_numbers","title":"get_chunk_page_numbers  <code>abstractmethod</code>","text":"<pre><code>get_chunk_page_numbers(doc_id: str, chunk_index: int) -&gt; Optional[tuple[int, int]]\n</code></pre> <p>Retrieve the page numbers of a specific chunk from a given document ID.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef get_chunk_page_numbers(self, doc_id: str, chunk_index: int) -&gt; Optional[tuple[int, int]]:\n    \"\"\"\n    Retrieve the page numbers of a specific chunk from a given document ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.get_chunk_text","title":"get_chunk_text  <code>abstractmethod</code>","text":"<pre><code>get_chunk_text(doc_id: str, chunk_index: int) -&gt; Optional[str]\n</code></pre> <p>Retrieve a specific chunk from a given document ID.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef get_chunk_text(self, doc_id: str, chunk_index: int) -&gt; Optional[str]:\n    \"\"\"\n    Retrieve a specific chunk from a given document ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.get_document","title":"get_document  <code>abstractmethod</code>","text":"<pre><code>get_document(doc_id: str) -&gt; Optional[FormattedDocument]\n</code></pre> <p>Retrieve all chunks from a given document ID.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef get_document(self, doc_id: str) -&gt; Optional[FormattedDocument]:\n    \"\"\"\n    Retrieve all chunks from a given document ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.get_document_summary","title":"get_document_summary  <code>abstractmethod</code>","text":"<pre><code>get_document_summary(doc_id: str, chunk_index: int) -&gt; Optional[str]\n</code></pre> <p>Retrieve the document summary of a specific chunk from a given document ID.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef get_document_summary(self, doc_id: str, chunk_index: int) -&gt; Optional[str]:\n    \"\"\"\n    Retrieve the document summary of a specific chunk from a given document ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.get_document_title","title":"get_document_title  <code>abstractmethod</code>","text":"<pre><code>get_document_title(doc_id: str, chunk_index: int) -&gt; Optional[str]\n</code></pre> <p>Retrieve the document title of a specific chunk from a given document ID.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef get_document_title(self, doc_id: str, chunk_index: int) -&gt; Optional[str]:\n    \"\"\"\n    Retrieve the document title of a specific chunk from a given document ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.get_is_visual","title":"get_is_visual  <code>abstractmethod</code>","text":"<pre><code>get_is_visual(doc_id: str, chunk_index: int) -&gt; Optional[bool]\n</code></pre> <p>Retrieve the is_visual flag of a specific chunk from a given document ID.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef get_is_visual(self, doc_id: str, chunk_index: int) -&gt; Optional[bool]:\n    \"\"\"\n    Retrieve the is_visual flag of a specific chunk from a given document ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.get_section_summary","title":"get_section_summary  <code>abstractmethod</code>","text":"<pre><code>get_section_summary(doc_id: str, chunk_index: int) -&gt; Optional[str]\n</code></pre> <p>Retrieve the section summary of a specific chunk from a given document ID.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef get_section_summary(self, doc_id: str, chunk_index: int) -&gt; Optional[str]:\n    \"\"\"\n    Retrieve the section summary of a specific chunk from a given document ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.get_section_title","title":"get_section_title  <code>abstractmethod</code>","text":"<pre><code>get_section_title(doc_id: str, chunk_index: int) -&gt; Optional[str]\n</code></pre> <p>Retrieve the section title of a specific chunk from a given document ID.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef get_section_title(self, doc_id: str, chunk_index: int) -&gt; Optional[str]:\n    \"\"\"\n    Retrieve the section title of a specific chunk from a given document ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.database.chunk.ChunkDB.remove_document","title":"remove_document  <code>abstractmethod</code>","text":"<pre><code>remove_document(doc_id: str) -&gt; None\n</code></pre> <p>Remove all chunks and metadata associated with a given document ID.</p> Source code in <code>dsrag/database/chunk/db.py</code> <pre><code>@abstractmethod\ndef remove_document(self, doc_id: str) -&gt; None:\n    \"\"\"\n    Remove all chunks and metadata associated with a given document ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#embedding-models","title":"Embedding Models","text":""},{"location":"api/knowledge_base/#dsrag.embedding.Embedding","title":"Embedding","text":"<pre><code>Embedding(dimension: Optional[int] = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> Source code in <code>dsrag/embedding.py</code> <pre><code>def __init__(self, dimension: Optional[int] = None):\n    self.dimension = dimension\n</code></pre>"},{"location":"api/knowledge_base/#rerankers","title":"Rerankers","text":""},{"location":"api/knowledge_base/#dsrag.reranker.Reranker","title":"Reranker","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api/knowledge_base/#llm-providers","title":"LLM Providers","text":""},{"location":"api/knowledge_base/#dsrag.llm.LLM","title":"LLM","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"api/knowledge_base/#dsrag.llm.LLM-functions","title":"Functions","text":""},{"location":"api/knowledge_base/#dsrag.llm.LLM.make_llm_call","title":"make_llm_call  <code>abstractmethod</code>","text":"<pre><code>make_llm_call(chat_messages: list[dict]) -&gt; str\n</code></pre> <p>Takes in chat_messages (OpenAI format) and returns the response from the LLM as a string.</p> Source code in <code>dsrag/llm.py</code> <pre><code>@abstractmethod\ndef make_llm_call(self, chat_messages: list[dict]) -&gt; str:\n    \"\"\"\n    Takes in chat_messages (OpenAI format) and returns the response from the LLM as a string.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#file-systems","title":"File Systems","text":""},{"location":"api/knowledge_base/#dsrag.dsparse.file_parsing.file_system.FileSystem","title":"FileSystem","text":"<pre><code>FileSystem(base_path: str)\n</code></pre> <p>               Bases: <code>ABC</code></p> Source code in <code>dsrag/dsparse/file_parsing/file_system.py</code> <pre><code>def __init__(self, base_path: str):\n    self.base_path = base_path\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.dsparse.file_parsing.file_system.FileSystem-functions","title":"Functions","text":""},{"location":"api/knowledge_base/#dsrag.dsparse.file_parsing.file_system.FileSystem.load_data","title":"load_data  <code>abstractmethod</code>","text":"<pre><code>load_data(kb_id: str, doc_id: str, data_name: str) -&gt; Optional[dict]\n</code></pre> <p>Load JSON data from a file Args:     kb_id: Knowledge base ID     doc_id: Document ID      data_name: Name of the data to load (e.g. \"elements\" for elements.json)</p> Source code in <code>dsrag/dsparse/file_parsing/file_system.py</code> <pre><code>@abstractmethod\ndef load_data(self, kb_id: str, doc_id: str, data_name: str) -&gt; Optional[dict]:\n    \"\"\"Load JSON data from a file\n    Args:\n        kb_id: Knowledge base ID\n        doc_id: Document ID \n        data_name: Name of the data to load (e.g. \"elements\" for elements.json)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.dsparse.file_parsing.file_system.FileSystem.load_page_content","title":"load_page_content  <code>abstractmethod</code>","text":"<pre><code>load_page_content(kb_id: str, doc_id: str, page_number: int) -&gt; Optional[str]\n</code></pre> <p>Load the text content of a page from its JSON file</p> Source code in <code>dsrag/dsparse/file_parsing/file_system.py</code> <pre><code>@abstractmethod\ndef load_page_content(self, kb_id: str, doc_id: str, page_number: int) -&gt; Optional[str]:\n    \"\"\"Load the text content of a page from its JSON file\"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.dsparse.file_parsing.file_system.FileSystem.load_page_content_range","title":"load_page_content_range  <code>abstractmethod</code>","text":"<pre><code>load_page_content_range(kb_id: str, doc_id: str, page_start: int, page_end: int) -&gt; list[str]\n</code></pre> <p>Load the text content for a range of pages</p> Source code in <code>dsrag/dsparse/file_parsing/file_system.py</code> <pre><code>@abstractmethod\ndef load_page_content_range(self, kb_id: str, doc_id: str, page_start: int, page_end: int) -&gt; list[str]:\n    \"\"\"Load the text content for a range of pages\"\"\"\n    pass\n</code></pre>"},{"location":"api/knowledge_base/#dsrag.dsparse.file_parsing.file_system.FileSystem.save_page_content","title":"save_page_content  <code>abstractmethod</code>","text":"<pre><code>save_page_content(kb_id: str, doc_id: str, page_number: int, content: str) -&gt; None\n</code></pre> <p>Save the text content of a page to a JSON file</p> Source code in <code>dsrag/dsparse/file_parsing/file_system.py</code> <pre><code>@abstractmethod\ndef save_page_content(self, kb_id: str, doc_id: str, page_number: int, content: str) -&gt; None:\n    \"\"\"Save the text content of a page to a JSON file\"\"\"\n    pass\n</code></pre>"},{"location":"community/contributing/","title":"Contributing to dsRAG","text":"<p>We welcome contributions from the community! Whether it's fixing bugs, improving documentation, or proposing new features, your contributions make dsRAG better for everyone.</p>"},{"location":"community/contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/your-username/dsRAG.git\ncd dsRAG\n</code></pre></li> <li>Create a new branch for your changes:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> </ol>"},{"location":"community/contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> </ol>"},{"location":"community/contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Make your changes in your feature branch</li> <li>Write or update tests as needed</li> <li>Update documentation to reflect your changes</li> <li>Run the test suite to ensure everything works:    <pre><code>python -m unittest discover\n</code></pre></li> </ol>"},{"location":"community/contributing/#code-style","title":"Code Style","text":"<p>We follow standard Python coding conventions: - Use PEP 8 style guide - Use meaningful variable and function names - Write docstrings for functions and classes - Keep functions focused and concise - Add comments for complex logic</p>"},{"location":"community/contributing/#documentation","title":"Documentation","text":"<p>When adding or modifying features: - Update docstrings for any modified functions/classes - Update relevant documentation in the <code>docs/</code> directory - Add examples if appropriate - Ensure documentation builds without errors:   <pre><code>mkdocs serve\n</code></pre></p>"},{"location":"community/contributing/#testing","title":"Testing","text":"<p>For new features or bug fixes: - Add appropriate unit tests in the <code>tests/</code> directory - Tests should subclass <code>unittest.TestCase</code> - Follow the existing test file naming pattern: <code>test_*.py</code> - Update existing tests if needed - Ensure all tests pass locally - Maintain or improve code coverage</p> <p>Example test structure: <pre><code>import unittest\nfrom dsrag import YourModule\n\nclass TestYourFeature(unittest.TestCase):\n    def setUp(self):\n        # Set up any test fixtures\n        pass\n\n    def test_your_feature(self):\n        # Test your new functionality\n        result = YourModule.your_function()\n        self.assertEqual(result, expected_value)\n\n    def tearDown(self):\n        # Clean up after tests\n        pass\n</code></pre></p>"},{"location":"community/contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li> <p>Commit your changes:     <pre><code>git add .\ngit commit -m \"Description of your changes\"\n</code></pre></p> </li> <li> <p>Push to your fork:     <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Open a Pull Request:</p> <ul> <li>Go to the dsRAG repository on GitHub</li> <li>Click \"Pull Request\"</li> <li>Select your feature branch</li> <li>Describe your changes and their purpose</li> <li>Reference any related issues</li> </ul> </li> </ol>"},{"location":"community/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Your PR should:</p> <ul> <li>Focus on a single feature or fix</li> <li>Include appropriate tests</li> <li>Update relevant documentation</li> <li>Follow the code style guidelines</li> <li>Include a clear description of the changes</li> <li>Reference any related issues</li> </ul>"},{"location":"community/contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with your contribution:</p> <ul> <li>Join our Discord</li> <li>Ask questions in the PR</li> <li>Tag maintainers if you need specific guidance</li> </ul>"},{"location":"community/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help others learn and grow</li> <li>Follow project maintainers' guidance</li> </ul> <p>Thank you for contributing to dsRAG! </p>"},{"location":"community/professional-services/","title":"Professional Services","text":""},{"location":"community/professional-services/#about-our-team","title":"About Our Team","text":"<p>The creators of dsRAG, Zach and Nick McCormick, run a specialized applied AI consulting firm. As former startup founders and YC alums, we bring a unique business and product-centric perspective to highly technical engineering projects.</p>"},{"location":"community/professional-services/#our-expertise","title":"Our Expertise","text":"<p>We specialize in:</p> <ul> <li>Building high-performance RAG-based applications</li> <li>Optimizing existing RAG systems</li> <li>Integrating advanced retrieval techniques</li> <li>Designing scalable AI architectures</li> </ul>"},{"location":"community/professional-services/#services-offered","title":"Services Offered","text":"<p>We provide:</p> <ul> <li>Advisory services</li> <li>Implementation work</li> <li>Performance optimization</li> <li>Custom feature development</li> <li>Technical architecture design</li> </ul>"},{"location":"community/professional-services/#getting-started","title":"Getting Started","text":"<p>If you need professional assistance with:</p> <ul> <li>Implementing dsRAG in your production environment</li> <li>Optimizing your RAG-based application</li> <li>Custom feature development</li> <li>Technical consultation</li> </ul> <p>Please fill out our contact form and we'll be in touch to discuss how we can help with your specific needs. </p>"},{"location":"community/support/","title":"Community Support","text":""},{"location":"community/support/#discord-community","title":"Discord Community","text":"<p>Join our Discord to:</p> <ul> <li>Ask questions about dsRAG</li> <li>Make suggestions for improvements</li> <li>Discuss potential contributions</li> <li>Connect with other developers</li> <li>Share your experiences</li> </ul>"},{"location":"community/support/#production-use-cases","title":"Production Use Cases","text":"<p>If you're using (or planning to use) dsRAG in production:</p> <ol> <li> <p>Please fill out our use case form</p> </li> <li> <p>This helps us:</p> <ul> <li>Understand how dsRAG is being used</li> <li>Prioritize new features</li> <li>Improve documentation</li> <li>Focus on the most important issues</li> </ul> </li> <li> <p>In return, you'll receive:</p> <ul> <li>Direct email contact with the creators</li> <li>Priority email support</li> <li>Early access to new features </li> </ul> </li> </ol>"},{"location":"concepts/chat/","title":"Chat","text":"<p>The Chat functionality in dsRAG provides a powerful way to interact with your knowledge bases through a conversational interface. It handles message history, knowledge base searching, and citation tracking automatically.</p>"},{"location":"concepts/chat/#overview","title":"Overview","text":"<p>The chat system works by:</p> <ol> <li>Maintaining a chat thread with message history</li> <li>Automatically generating relevant search queries based on user input</li> <li>Searching knowledge bases for relevant information</li> <li>Generating responses with citations to source materials</li> </ol>"},{"location":"concepts/chat/#defining-where-to-store-chat-history","title":"Defining Where to Store Chat History","text":"<p>Chat threads in dsRAG need to be persisted somewhere so that conversations can continue across multiple interactions. You'll need to provide your own implementation of the <code>ChatThreadDB</code> class to handle this storage. This allows you to store chat threads in whatever database or storage system works best for your application.</p> <p>The <code>ChatThreadDB</code> interface defines methods for: - Creating new chat threads - Retrieving existing threads - Adding interactions to threads - Managing thread metadata and configuration</p> <p>You'll need to initialize your chat thread storage before creating any new chat threads or retrieving responses.</p> <p>There are two implementations of the <code>ChatThreadDB</code> interface included: - <code>BasicChatThreadDB</code>: A basic implementation that stores chat threads in a JSON file - <code>SQLiteChatThreadDB</code>: A SQLite implementation that stores chat threads in a SQLite database</p>"},{"location":"concepts/chat/#creating-a-chat-thread","title":"Creating a Chat Thread","text":"<p>To start a conversation, first create a chat thread:</p> <pre><code>from dsrag.chat.chat import create_new_chat_thread\nfrom dsrag.database.chat_thread.sqlite_db import SQLiteChatThreadDB\n\n# Configure chat parameters\nchat_params = {\n    \"kb_ids\": [\"my_knowledge_base\"],  # List of knowledge base IDs to use\n    \"model\": \"gpt-4o\",                 # LLM model to use\n    \"temperature\": 0.2,               # Response creativity (0.0-1.0)\n    \"system_message\": \"You are a helpful assistant specialized in technical documentation\",\n    \"target_output_length\": \"medium\"  # \"short\", \"medium\", or \"long\"\n}\n\n# Initialize chat thread database (SQLite in this case)\nchat_thread_db = SQLiteChatThreadDB()\n\n# Create the thread\nthread_id = create_new_chat_thread(chat_params, chat_thread_db)\n</code></pre>"},{"location":"concepts/chat/#getting-responses","title":"Getting Responses","text":"<p>Once you have a thread, you can send messages and get responses:</p> <pre><code>from dsrag.chat.chat import get_chat_thread_response\nfrom dsrag.chat.chat_types import ChatResponseInput\n\n# Create input with optional metadata filter\nresponse_input = ChatResponseInput(\n    user_input=\"What are the key features of XYZ product?\",\n    metadata_filter={\n        \"field\": \"doc_id\",\n        \"operator\": \"equals\",\n        \"value\": \"user_manual\"\n    }\n)\n\n# Create the knowledge base instances\nknowledge_bases = {\n    \"my_knowledge_base\": KnowledgeBase(kb_id=\"my_knowledge_base\")\n}\n\n# Get response\nresponse = get_chat_thread_response(\n    thread_id=thread_id,\n    get_response_input=response_input,\n    chat_thread_db=chat_thread_db,\n    knowledge_bases=knowledge_bases  # Dictionary of your knowledge base instances\n)\n\n# Access the response content and citations\nprint(response[\"model_response\"][\"content\"])\nfor citation in response[\"model_response\"][\"citations\"]:\n    print(f\"Source: {citation['doc_id']}, Page: {citation['page_number']}\")\n</code></pre>"},{"location":"concepts/chat/#chat-thread-parameters","title":"Chat Thread Parameters","text":"<p>The chat thread parameters dictionary supports several configuration options:</p> <ul> <li><code>kb_ids</code>: List of knowledge base IDs to search</li> <li><code>model</code>: LLM model to use (e.g., \"gpt-4\")</li> <li><code>temperature</code>: Controls response randomness (0.0-1.0)</li> <li><code>system_message</code>: Custom instructions for the LLM</li> <li><code>auto_query_model</code>: Model to use for generating search queries</li> <li><code>auto_query_guidance</code>: Custom guidance for query generation</li> <li><code>target_output_length</code>: Desired response length (\"short\", \"medium\", \"long\")</li> <li><code>max_chat_history_tokens</code>: Maximum tokens to keep in chat history</li> </ul>"},{"location":"concepts/chat/#response-structure","title":"Response Structure","text":"<p>Chat responses include:</p> <ul> <li>User input with timestamp</li> <li>Model response with citations</li> <li>Search queries used</li> <li>Relevant segments found</li> </ul>"},{"location":"concepts/chat/#best-practices","title":"Best Practices","text":"<ol> <li>Set appropriate <code>target_output_length</code> based on your use case</li> <li>Use <code>system_message</code> to guide the LLM's behavior</li> <li>Configure <code>max_chat_history_tokens</code> based on your needs</li> <li>Use metadata filters to focus searches on relevant documents</li> <li>Monitor and adjust <code>temperature</code> based on desired response creativity</li> </ol>"},{"location":"concepts/citations/","title":"Citations","text":"<p>dsRAG's citation system ensures that responses are grounded in your knowledge base content and provides transparency about information sources. Citations are included automatically in chat responses.</p>"},{"location":"concepts/citations/#overview","title":"Overview","text":"<p>Citations in dsRAG:</p> <ul> <li>Track the source of information in responses</li> <li>Include document IDs and page numbers</li> <li>Provide exact quoted text from sources</li> </ul>"},{"location":"concepts/citations/#citation-structure","title":"Citation Structure","text":"<p>Each citation includes:</p> <ul> <li><code>doc_id</code>: Unique identifier of the source document</li> <li><code>page_number</code>: Page where the information was found (if available)</li> <li><code>cited_text</code>: Exact text containing the cited information</li> <li><code>kb_id</code>: ID of the knowledge base containing the document</li> </ul>"},{"location":"concepts/citations/#working-with-citations","title":"Working with Citations","text":"<p>Citations are automatically included in chat responses:</p> <pre><code>from dsrag.chat.chat import get_chat_thread_response\nfrom dsrag.chat.chat_types import ChatResponseInput\nfrom dsrag.database.chat_thread.sqlite_db import SQLiteChatThreadDB\n\n# Create the knowledge base instances\nknowledge_bases = {\n    \"my_knowledge_base\": KnowledgeBase(kb_id=\"my_knowledge_base\")\n}\n\n# Initialize database and get response\nchat_thread_db = SQLiteChatThreadDB()\nresponse = get_chat_thread_response(\n    thread_id=thread_id,\n    get_response_input=ChatResponseInput(\n        user_input=\"What is the system architecture?\"\n    ),\n    chat_thread_db=chat_thread_db,\n    knowledge_bases=knowledge_bases\n)\n\n# Access citations\ncitations = response[\"model_response\"][\"citations\"]\nfor citation in citations:\n    print(f\"\"\"\nSource: {citation['doc_id']}\nPage: {citation['page_number']}\nText: {citation['cited_text']}\nKnowledge Base: {citation['kb_id']}\n\"\"\")\n</code></pre>"},{"location":"concepts/citations/#technical-details","title":"Technical Details","text":"<p>Citations are managed through the <code>ResponseWithCitations</code> model:</p> <pre><code>from dsrag.chat.citations import ResponseWithCitations, Citation\n\nresponse = ResponseWithCitations(\n    response=\"The system architecture is...\",\n    citations=[\n        Citation(\n            doc_id=\"arch-doc-v1\",\n            page_number=12,\n            cited_text=\"The system uses a microservices architecture\"\n        )\n    ]\n)\n</code></pre> <p>This structured format ensures consistent citation handling throughout the system.</p>"},{"location":"concepts/components/","title":"Components","text":"<p>There are six key components that define the configuration of a KnowledgeBase. Each component is customizable, with several built-in options available.</p>"},{"location":"concepts/components/#vectordb","title":"VectorDB","text":"<p>The VectorDB component stores embedding vectors and associated metadata.</p> <p>Available options:</p> <ul> <li><code>BasicVectorDB</code></li> <li><code>WeaviateVectorDB</code></li> <li><code>ChromaDB</code></li> <li><code>QdrantVectorDB</code></li> <li><code>MilvusDB</code></li> <li><code>PineconeDB</code></li> </ul>"},{"location":"concepts/components/#chunkdb","title":"ChunkDB","text":"<p>The ChunkDB stores the content of text chunks in a nested dictionary format, keyed on <code>doc_id</code> and <code>chunk_index</code>. This is used by RSE to retrieve the full text associated with specific chunks.</p> <p>Available options:</p> <ul> <li><code>BasicChunkDB</code></li> <li><code>SQLiteDB</code></li> </ul>"},{"location":"concepts/components/#embedding","title":"Embedding","text":"<p>The Embedding component defines the embedding model used for vectorizing text.</p> <p>Available options:</p> <ul> <li><code>OpenAIEmbedding</code></li> <li><code>CohereEmbedding</code></li> <li><code>VoyageAIEmbedding</code></li> <li><code>OllamaEmbedding</code></li> </ul>"},{"location":"concepts/components/#reranker","title":"Reranker","text":"<p>The Reranker component provides more accurate ranking of chunks after vector database search and before RSE. This is optional but highly recommended.</p> <p>Available options:</p> <ul> <li><code>CohereReranker</code></li> <li><code>VoyageReranker</code></li> <li><code>NoReranker</code></li> </ul>"},{"location":"concepts/components/#llm","title":"LLM","text":"<p>The LLM component is used in AutoContext for:</p> <ul> <li>Document title generation</li> <li>Document summarization</li> <li>Section summarization</li> </ul> <p>Available options:</p> <ul> <li><code>OpenAIChatAPI</code></li> <li><code>AnthropicChatAPI</code></li> <li><code>OllamaChatAPI</code></li> </ul>"},{"location":"concepts/components/#filesystem","title":"FileSystem","text":"<p>The FileSystem component defines where to save PDF images and extracted elementsfor VLM file parsing.</p> <p>Available options:</p> <ul> <li><code>LocalFileSystem</code></li> <li><code>S3FileSystem</code></li> </ul>"},{"location":"concepts/components/#localfilesystem-configuration","title":"LocalFileSystem Configuration","text":"<p>Only requires a <code>base_path</code> parameter to define where files will be stored on the system.</p>"},{"location":"concepts/components/#s3filesystem-configuration","title":"S3FileSystem Configuration","text":"<p>Requires the following parameters:</p> <ul> <li><code>base_path</code>: Used when downloading files from S3</li> <li><code>bucket_name</code>: S3 bucket name</li> <li><code>region_name</code>: AWS region</li> <li><code>access_key</code>: AWS access key</li> <li><code>access_secret</code>: AWS secret key</li> </ul> <p>Note: Files must be stored locally temporarily for use in the retrieval system, even when using S3. </p>"},{"location":"concepts/config/","title":"Configuration","text":"<p>dsRAG uses several configuration dictionaries to organize its many parameters. These configs can be passed to different methods of the KnowledgeBase class.</p>"},{"location":"concepts/config/#document-addition-configs","title":"Document Addition Configs","text":"<p>The following configuration dictionaries can be passed to the <code>add_document</code> method:</p>"},{"location":"concepts/config/#autocontext-config","title":"AutoContext Config","text":"<pre><code>auto_context_config = {\n    \"use_generated_title\": bool,  # whether to use an LLM-generated title if no title is provided (default: True)\n    \"document_title_guidance\": str,  # Additional guidance for generating the document title (default: \"\")\n    \"get_document_summary\": bool,  # whether to get a document summary (default: True)\n    \"document_summarization_guidance\": str,  # Additional guidance for summarizing the document (default: \"\")\n    \"get_section_summaries\": bool,  # whether to get section summaries (default: False)\n    \"section_summarization_guidance\": str  # Additional guidance for summarizing the sections (default: \"\")\n}\n</code></pre>"},{"location":"concepts/config/#file-parsing-config","title":"File Parsing Config","text":"<pre><code>file_parsing_config = {\n    \"use_vlm\": bool,  # whether to use VLM for parsing the file (default: False)\n    \"vlm_config\": {\n        \"provider\": str,  # the VLM provider to use (default: \"gemini\", \"vertex_ai\" is also supported)\n        \"model\": str,  # the VLM model to use (default: \"gemini-2.0-flash\")\n        \"project_id\": str,  # GCP project ID (only required for \"vertex_ai\")\n        \"location\": str,  # GCP location (only required for \"vertex_ai\")\n        \"save_path\": str,  # path to save intermediate files during VLM processing\n        \"exclude_elements\": list,  # element types to exclude (default: [\"Header\", \"Footer\"])\n        \"images_already_exist\": bool  # whether images are pre-extracted (default: False)\n    },\n    \"always_save_page_images\": bool  # save page images even if VLM is not used (default: False)\n}\n</code></pre>"},{"location":"concepts/config/#semantic-sectioning-config","title":"Semantic Sectioning Config","text":"<pre><code>semantic_sectioning_config = {\n    \"llm_provider\": str,  # LLM provider (default: \"openai\", \"anthropic\" and \"gemini\" are also supported)\n    \"model\": str,  # LLM model to use (default: \"gpt-4o-mini\")\n    \"use_semantic_sectioning\": bool  # if False, skip semantic sectioning (default: True)\n}\n</code></pre>"},{"location":"concepts/config/#chunking-config","title":"Chunking Config","text":"<pre><code>chunking_config = {\n    \"chunk_size\": int,  # maximum characters per chunk (default: 800)\n    \"min_length_for_chunking\": int  # minimum text length to allow chunking (default: 2000)\n}\n</code></pre>"},{"location":"concepts/config/#query-config","title":"Query Config","text":"<p>The following configuration dictionary can be passed to the <code>query</code> method:</p>"},{"location":"concepts/config/#rse-parameters","title":"RSE Parameters","text":"<pre><code>rse_params = {\n    \"max_length\": int,  # maximum segment length in chunks (default: 15)\n    \"overall_max_length\": int,  # maximum total length of all segments (default: 30)\n    \"minimum_value\": float,  # minimum relevance value for segments (default: 0.5)\n    \"irrelevant_chunk_penalty\": float,  # penalty for irrelevant chunks (0-1) (default: 0.18)\n    \"overall_max_length_extension\": int,  # length increase per additional query (default: 5)\n    \"decay_rate\": float,  # rate at which relevance decays (default: 30)\n    \"top_k_for_document_selection\": int,  # maximum number of documents to consider (default: 10)\n    \"chunk_length_adjustment\": bool  # whether to scale by chunk length (default: True)\n}\n</code></pre>"},{"location":"concepts/config/#metadata-query-filters","title":"Metadata Query Filters","text":"<p>Some vector databases (currently only ChromaDB) support metadata filtering during queries. This allows for more controlled document selection.</p> <p>Example metadata filter format: <pre><code>metadata_filter = {\n    \"field\": str,  # The metadata field to filter by\n    \"operator\": str,  # One of: 'equals', 'not_equals', 'in', 'not_in', \n                     # 'greater_than', 'less_than', 'greater_than_equals', 'less_than_equals'\n    \"value\": str | int | float | list  # If list, all items must be same type\n}\n</code></pre></p> <p>Example usage: <pre><code># Filter with \"equals\" operator\nmetadata_filter = {\n    \"field\": \"doc_id\",\n    \"operator\": \"equals\",\n    \"value\": \"test_id_1\"\n}\n\n# Filter with \"in\" operator\nmetadata_filter = {\n    \"field\": \"doc_id\",\n    \"operator\": \"in\",\n    \"value\": [\"test_id_1\", \"test_id_2\"]\n}\n</code></pre></p>"},{"location":"concepts/knowledge-bases/","title":"Knowledge Bases","text":"<p>A knowledge base in dsRAG is a searchable collection of documents that can be queried to find relevant information. The <code>KnowledgeBase</code> class handles document processing, storage, and retrieval.</p>"},{"location":"concepts/knowledge-bases/#creating-a-knowledge-base","title":"Creating a Knowledge Base","text":"<p>To create a knowledge base:</p> <pre><code>from dsrag.knowledge_base import KnowledgeBase\n\n# Create a basic knowledge base\nkb = KnowledgeBase(\n    kb_id=\"my_kb\",\n    title=\"Product Documentation\",\n    description=\"Technical documentation for XYZ product\"\n)\n\n# Or with custom configuration\nkb = KnowledgeBase(\n    kb_id=\"my_kb\",\n    storage_directory=\"path/to/storage\",  # Where to store KB data\n    embedding_model=custom_embedder,      # Custom embedding model\n    reranker=custom_reranker,            # Custom reranking model\n    vector_db=custom_vector_db,          # Custom vector database\n    chunk_db=custom_chunk_db            # Custom chunk database\n)\n</code></pre>"},{"location":"concepts/knowledge-bases/#adding-documents","title":"Adding Documents","text":"<p>Documents can be added from text or files:</p> <pre><code># Add from text\nkb.add_document(\n    doc_id=\"intro-guide\",\n    text=\"This is the introduction guide...\",\n    document_title=\"Introduction Guide\",\n    metadata={\"type\": \"guide\", \"version\": \"1.0\"}\n)\n\n# Add from file\nkb.add_document(\n    doc_id=\"user-manual\",\n    file_path=\"path/to/manual.pdf\",\n    metadata={\"type\": \"manual\", \"department\": \"engineering\"}\n)\n\n# Add with advanced configuration\nkb.add_document(\n    doc_id=\"technical-spec\",\n    file_path=\"path/to/spec.pdf\",\n    file_parsing_config={\n        \"use_vlm\": True,                # Use vision language model for PDFs\n        \"always_save_page_images\": True  # Save page images for visual content\n    },\n    chunking_config={\n        \"chunk_size\": 800,              # Characters per chunk\n        \"min_length_for_chunking\": 2000 # Minimum length to chunk\n    },\n    auto_context_config={\n        \"use_generated_title\": True,    # Generate title if not provided\n        \"get_document_summary\": True    # Generate document summary\n    }\n)\n</code></pre>"},{"location":"concepts/knowledge-bases/#querying-the-knowledge-base","title":"Querying the Knowledge Base","text":"<p>Search the knowledge base for relevant information:</p> <pre><code># Simple query\nresults = kb.query(\n    search_queries=[\"How to configure the system?\"]\n)\n\n# Advanced query with filtering and parameters\nresults = kb.query(\n    search_queries=[\n        \"System configuration steps\",\n        \"Configuration prerequisites\"\n    ],\n    metadata_filter={\n        \"field\": \"doc_id\",\n        \"operator\": \"equals\",\n        \"value\": \"user_manual\"\n    },\n    rse_params=\"precise\",  # Use preset RSE parameters\n    return_mode=\"text\"     # Return text content\n)\n\n# Process results\nfor segment in results:\n    print(f\"\"\"\nDocument: {segment['doc_id']}\nPages: {segment['segment_page_start']} - {segment['segment_page_end']}\nContent: {segment['content']}\nRelevance: {segment['score']}\n\"\"\")\n</code></pre>"},{"location":"concepts/knowledge-bases/#rse-parameters","title":"RSE Parameters","text":"<p>The Relevant Segment Extraction (RSE) system can be tuned using different parameter presets: - <code>\"balanced\"</code>: Default preset balancing precision and comprehensiveness - <code>\"precise\"</code>: Favors shorter, more focused segments - <code>\"comprehensive\"</code>: Returns longer segments with more context</p> <p>Or configure custom RSE parameters: <pre><code>results = kb.query(\n    search_queries=[\"system requirements\"],\n    rse_params={\n        \"max_length\": 5,                # Max segments length (in number of chunks)\n        \"overall_max_length\": 20,       # Total length limit across all segments (in number of chunks)\n        \"minimum_value\": 0.5,           # Minimum relevance score\n        \"irrelevant_chunk_penalty\": 0.2 # Penalty for irrelevant chunks in a segment - higher penalty leads to shorter segments\n    }\n)\n</code></pre></p>"},{"location":"concepts/knowledge-bases/#metadata-query-filters","title":"Metadata Query Filters","text":"<p>Certain vector DBs support metadata filtering when running a query (currently only ChromaDB). This allows you to have more control over what document(s) get searched. A common use case would be asking questions about a single document in a knowledge base, in which case you would supply the <code>doc_id</code> as a metadata filter.</p> <p>The metadata filter should be a dictionary with the following structure:</p> <pre><code>metadata_filter = {\n    \"field\": \"doc_id\",      # The metadata field to filter on\n    \"operator\": \"equals\",   # The comparison operator\n    \"value\": \"doc123\"      # The value to compare against\n}\n</code></pre> <p>Supported operators: - <code>equals</code> - <code>not_equals</code>  - <code>in</code> - <code>not_in</code> - <code>greater_than</code> - <code>less_than</code> - <code>greater_than_equals</code> - <code>less_than_equals</code></p> <p>For operators that take multiple values (<code>in</code> and <code>not_in</code>), the value should be a list where all items are of the same type (string, integer, or float).</p> <p>Example usage: <pre><code># Query a specific document\nresults = kb.query(\n    search_queries=[\"system requirements\"],\n    metadata_filter={\n        \"field\": \"doc_id\",\n        \"operator\": \"equals\",\n        \"value\": \"technical_spec_v1\"\n    }\n)\n\n# Query documents from multiple departments\nresults = kb.query(\n    search_queries=[\"security protocols\"],\n    metadata_filter={\n        \"field\": \"department\",\n        \"operator\": \"in\", \n        \"value\": [\"security\", \"compliance\"]\n    }\n)\n</code></pre></p>"},{"location":"concepts/overview/","title":"Architecture Overview","text":"<p>dsRAG is built around three key methods that improve performance over vanilla RAG systems:</p> <ol> <li>Semantic sectioning</li> <li>AutoContext</li> <li>Relevant Segment Extraction (RSE)</li> </ol>"},{"location":"concepts/overview/#key-methods","title":"Key Methods","text":""},{"location":"concepts/overview/#semantic-sectioning","title":"Semantic Sectioning","text":"<p>Semantic sectioning uses an LLM to break a document into cohesive sections. The process works as follows:</p> <ol> <li>The document is annotated with line numbers</li> <li>An LLM identifies the starting and ending lines for each \"semantically cohesive section\"</li> <li>Sections typically range from a few paragraphs to a few pages long</li> <li>Sections are broken into smaller chunks if needed</li> <li>The LLM generates descriptive titles for each section</li> <li>Section titles are used in contextual chunk headers created by AutoContext</li> </ol> <p>This process provides additional context to the ranking models (embeddings and reranker), enabling better retrieval.</p>"},{"location":"concepts/overview/#autocontext-contextual-chunk-headers","title":"AutoContext (Contextual Chunk Headers)","text":"<p>AutoContext creates contextual chunk headers that contain:</p> <ul> <li>Document-level context</li> <li>Section-level context</li> </ul> <p>These headers are prepended to chunks before embedding. Benefits include:</p> <ul> <li>More accurate and complete representation of text content and meaning</li> <li>Dramatic improvement in retrieval quality</li> <li>Reduced rate of irrelevant results</li> <li>Reduced LLM misinterpretation in downstream applications</li> </ul>"},{"location":"concepts/overview/#relevant-segment-extraction-rse","title":"Relevant Segment Extraction (RSE)","text":"<p>RSE is a query-time post-processing step that:</p> <ol> <li>Takes clusters of relevant chunks</li> <li>Intelligently combines them into longer sections (segments)</li> <li>Provides better context to the LLM than individual chunks</li> </ol> <p>RSE is particularly effective for:</p> <ul> <li>Complex questions where answers span multiple chunks</li> <li>Adapting the context length based on query type</li> <li>Maintaining coherent context while avoiding irrelevant information</li> </ul>"},{"location":"concepts/overview/#document-processing-flow","title":"Document Processing Flow","text":"<ol> <li>Documents \u2192 VLM file parsing</li> <li>Semantic sectioning</li> <li>Chunking</li> <li>AutoContext</li> <li>Embedding</li> <li>Chunk and vector database upsert</li> </ol>"},{"location":"concepts/overview/#query-processing-flow","title":"Query Processing Flow","text":"<ol> <li>Queries</li> <li>Vector database search</li> <li>Reranking</li> <li>RSE</li> <li>Results</li> </ol> <p>For more detailed information about specific components and configuration options, please refer to: - Components Documentation - Configuration Options - Knowledge Base Details </p>"},{"location":"concepts/vlm/","title":"VLM File Parsing","text":"<p>dsRAG supports Vision Language Model (VLM) integration for enhanced PDF parsing capabilities. This feature is particularly useful for documents with complex layouts, tables, diagrams, or other visual elements that traditional text extraction might miss.</p>"},{"location":"concepts/vlm/#overview","title":"Overview","text":"<p>When VLM parsing is enabled:</p> <ol> <li>The PDF is converted to images (one per page)</li> <li>Each page image is analyzed by the VLM to identify and extract text and visual elements</li> <li>The extracted elements are converted to a structured format with page numbers preserved</li> </ol>"},{"location":"concepts/vlm/#supported-providers","title":"Supported Providers","text":"<p>Currently supported VLM providers:</p> <ul> <li>Google's Gemini (default)</li> <li>Google Cloud Vertex AI</li> </ul>"},{"location":"concepts/vlm/#configuration","title":"Configuration","text":"<p>To use VLM for file parsing, configure the <code>file_parsing_config</code> when adding documents:</p> <pre><code>file_parsing_config = {\n    \"use_vlm\": True,  # Enable VLM parsing\n    \"vlm_config\": {\n        # VLM Provider (required)\n        \"provider\": \"gemini\",  # or \"vertex_ai\"\n\n        # Model name (optional - defaults based on provider)\n        \"model\": \"gemini-2.0-flash\",  # default for Gemini\n\n        # For Vertex AI, additional required fields:\n        \"project_id\": \"your-gcp-project-id\",\n        \"location\": \"us-central1\",\n\n        # Elements to exclude from parsing (optional)\n        \"exclude_elements\": [\"Header\", \"Footer\"],\n\n        # Whether images are pre-extracted (optional)\n        \"images_already_exist\": False\n    },\n\n    # Save page images even if VLM unused (optional)\n    \"always_save_page_images\": False\n}\n\n# Add document with VLM parsing (must be a PDF file)\nkb.add_document(\n    doc_id=\"my_document\",\n    file_path=\"path/to/document.pdf\",  # Only PDF files are supported with VLM\n    file_parsing_config=file_parsing_config\n)\n</code></pre>"},{"location":"concepts/vlm/#element-types","title":"Element Types","text":"<p>The VLM is prompted to categorize page content into the following element types:</p> <p>Text Elements:</p> <ul> <li>NarrativeText: Main text content including paragraphs, lists, and titles</li> <li>Header: Page header content (typically at top of page)</li> <li>Footnote: References or notes at bottom of content</li> <li>Footer: Page footer content (at very bottom of page)</li> </ul> <p>Visual Elements:</p> <ul> <li>Figure: Charts, graphs, and diagrams with associated titles and legends</li> <li>Image: Photos, illustrations, and other visual content</li> <li>Table: Tabular data arrangements with titles and captions</li> <li>Equation: Mathematical formulas and expressions</li> </ul> <p>By default, Header and Footer elements are excluded from parsing as they rarely contain valuable information and can break the flow between pages. You can modify which elements to exclude using the <code>exclude_elements</code> configuration option.</p>"},{"location":"concepts/vlm/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use VLM parsing for documents with:</p> <ul> <li>Complex layouts</li> <li>Important visual elements</li> <li>Tables that need precise formatting</li> <li>Mathematical formulas</li> <li>Scanned documents that need OCR</li> </ul> </li> <li> <p>Consider traditional parsing for:</p> <ul> <li>Simple text documents</li> <li>Documents where visual layout isn't critical</li> <li>Large volumes of documents (VLM parsing is slower and more expensive)</li> </ul> </li> <li> <p>Configure <code>exclude_elements</code> to ignore irrelevant elements like headers/footers</p> </li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installing-dsrag","title":"Installing dsRAG","text":"<p>Install the package using pip:</p> <pre><code>pip install dsrag\n</code></pre> <p>If you want to use VLM file parsing, you will need one non-Python dependency: poppler. This is used for converting PDFs to images. On MacOS, you can install it using Homebrew:</p> <pre><code>brew install poppler\n</code></pre>"},{"location":"getting-started/installation/#third-party-dependencies","title":"Third-party dependencies","text":"<p>To get the most out of dsRAG, you'll need a few third-party API keys set as environment variables. Here are the most important ones:</p> <ul> <li><code>OPENAI_API_KEY</code>: Recommended for embeddings, AutoContext, and semantic sectioning</li> <li><code>CO_API_KEY</code>: Recommended for reranking</li> <li><code>GEMINI_API_KEY</code>: Recommended for VLM file parsing</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you get started with dsRAG quickly. We'll cover the basics of creating a knowledge base and querying it.</p>"},{"location":"getting-started/quickstart/#setting-up","title":"Setting Up","text":"<p>First, make sure you have the necessary API keys set as environment variables: - <code>OPENAI_API_KEY</code> for embeddings and AutoContext - <code>CO_API_KEY</code> for reranking with Cohere</p> <p>If you don't have both of these available, you can use the OpenAI-only configuration or local-only configuration below.</p>"},{"location":"getting-started/quickstart/#creating-a-knowledge-base","title":"Creating a Knowledge Base","text":"<p>Create a new knowledge base and add documents to it:</p> <pre><code>from dsrag.knowledge_base import KnowledgeBase\n\n# Create a knowledge base\nkb = KnowledgeBase(kb_id=\"my_knowledge_base\")\n\n# Add documents\nkb.add_document(\n    doc_id=\"user_manual\",  # Use a meaningful ID if possible\n    file_path=\"path/to/your/document.pdf\",\n    document_title=\"User Manual\",  # Optional but recommended\n    metadata={\"type\": \"manual\"}    # Optional metadata\n)\n</code></pre> <p>The KnowledgeBase object persists to disk automatically, so you don't need to explicitly save it.</p>"},{"location":"getting-started/quickstart/#querying-the-knowledge-base","title":"Querying the Knowledge Base","text":"<p>Once you have created a knowledge base, you can load it by its <code>kb_id</code> and query it:</p> <pre><code>from dsrag.knowledge_base import KnowledgeBase\n\n# Load the knowledge base\nkb = KnowledgeBase(\"my_knowledge_base\")\n\n# You can query with multiple search queries\nsearch_queries = [\n    \"What are the main topics covered?\",\n    \"What are the key findings?\"\n]\n\n# Get results\nresults = kb.query(search_queries)\nfor segment in results:\n    print(segment)\n</code></pre>"},{"location":"getting-started/quickstart/#using-openai-only-configuration","title":"Using OpenAI-Only Configuration","text":"<p>If you prefer to use only OpenAI services (without Cohere), you can customize the configuration:</p> <pre><code>from dsrag.llm import OpenAIChatAPI\nfrom dsrag.reranker import NoReranker\n\n# Configure components\nllm = OpenAIChatAPI(model='gpt-4o-mini')\nreranker = NoReranker()\n\n# Create knowledge base with custom configuration\nkb = KnowledgeBase(\n    kb_id=\"my_knowledge_base\",\n    reranker=reranker,\n    auto_context_model=llm\n)\n\n# Add documents\nkb.add_document(\n    doc_id=\"user_manual\",  # Use a meaningful ID\n    file_path=\"path/to/your/document.pdf\",\n    document_title=\"User Manual\",  # Optional but recommended\n    metadata={\"type\": \"manual\"}    # Optional metadata\n)\n</code></pre>"},{"location":"getting-started/quickstart/#local-only-configuration","title":"Local-only configuration","text":"<p>If you don't want to use any third-party services, you can configure dsRAG to run fully locally using Ollama. There will be a couple limitations: - You will not be able to use VLM file parsing - You will not be able to use semantic sectioning - You will not be able to use a reranker, unless you implement your own custom Reranker class</p> <pre><code>from dsrag.llm import OllamaChatAPI\nfrom dsrag.reranker import NoReranker\nfrom dsrag.embedding import OllamaEmbedding\n\nllm = OllamaChatAPI(model=\"llama3.1:8b\")\nreranker = NoReranker()\nembedding = OllamaEmbedding(model=\"nomic-embed-text\")\n\nkb = KnowledgeBase(\n    kb_id=\"my_knowledge_base\",\n    reranker=reranker,\n    auto_context_model=llm,\n    embedding_model=embedding\n)\n\n# Disable semantic sectioning\nsemantic_sectioning_config = {\n    \"use_semantic_sectioning\": False,\n}\n\n# Add documents\nkb.add_document(\n    doc_id=\"user_manual\",\n    file_path=\"path/to/your/document.pdf\",\n    document_title=\"User Manual\",\n    semantic_sectioning_config=semantic_sectioning_config\n)\n</code></pre>"}]}